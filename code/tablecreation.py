# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k5MO1AjB2yoe6f5w2-134Iukc0SsioKz
"""

import os, re, glob, zipfile, shutil, hashlib
import numpy as np
import pandas as pd

DEFAULT_SEARCH_DIRS = ["/content", "/content/drive/MyDrive", "/mnt/data"]
OUT_DIR = "/content/output_insurance_dwh"
EXTRACT_DIR = "/content/insurance_extracted"
os.makedirs(OUT_DIR, exist_ok=True)

pd.set_option("display.max_columns", 200)

CANON_RAW_COLS_26 = [
    "Customer ID", "Customer Name", "Customer_Segment", "Maritial_Status", "Gender", "DOB",
    "Effective_Start_Dt", "Effective_End_Dt",
    "Policy_Type_Id", "Policy_Type", "Policy_Type_Desc",
    "Policy_Id", "Policy_Name", "Premium_Amt", "Policy_Term",
    "Policy_Start_Dt", "Policy_End_Dt", "Next_Premium_Dt", "Actual_Premium_Paid_Dt",
    "Country", "Region", "State or Province", "City", "Postal Code",
    "Total_Policy_Amt", "Premium_Amt_Paid_TillDate"
]

RAW_TO_SNAKE = {
    "Customer ID": "customer_id",
    "Customer Name": "customer_name",
    "Customer_Segment": "customer_segment",
    "Maritial_Status": "marital_status",
    "Gender": "gender",
    "DOB": "dob",
    "Effective_Start_Dt": "effective_start_dt",
    "Effective_End_Dt": "effective_end_dt",
    "Policy_Type_Id": "policy_type_id",
    "Policy_Type": "policy_type",
    "Policy_Type_Desc": "policy_type_desc",
    "Policy_Id": "policy_id",                 # NOTE: keep string
    "Policy_Name": "policy_name",
    "Premium_Amt": "premium_amt",
    "Policy_Term": "policy_term",
    "Policy_Start_Dt": "policy_start_dt",
    "Policy_End_Dt": "policy_end_dt",
    "Next_Premium_Dt": "next_premium_dt",
    "Actual_Premium_Paid_Dt": "actual_premium_paid_dt",
    "Country": "country",
    "Region": "region",
    "State or Province": "state_or_province",
    "City": "city",
    "Postal Code": "postal_code",
    "Total_Policy_Amt": "total_policy_amt",
    "Premium_Amt_Paid_TillDate": "premium_amt_paid_tilldate",
}

def find_zip_files():
    zips = []
    for d in DEFAULT_SEARCH_DIRS:
        if os.path.exists(d):
            zips += glob.glob(os.path.join(d, "Insurance_details_US_*_day.zip"))
    zips = sorted(list(set(zips)))
    if not zips:
        raise FileNotFoundError("Upload 4 region zip files in /content")
    return zips

def extract_all_zips(zip_paths, extract_base):
    shutil.rmtree(extract_base, ignore_errors=True)
    os.makedirs(extract_base, exist_ok=True)
    for zp in zip_paths:
        name = os.path.basename(zp)
        m = re.search(r"Insurance_details_US_(\w+)_day\.zip", name, flags=re.IGNORECASE)
        region = m.group(1) if m else "UNKNOWN"
        out = os.path.join(extract_base, region)
        os.makedirs(out, exist_ok=True)
        with zipfile.ZipFile(zp, "r") as z:
            z.extractall(out)

def read_csv_safe(path):
    for sep in [",", "|", ";", "\t"]:
        try:
            df = pd.read_csv(path, sep=sep, engine="python")
            if df.shape[1] > 1:
                return df
        except Exception:
            pass
    raise ValueError(f"Cannot read CSV: {path}")

def norm_space(s: pd.Series):
    return (s.astype(str)
              .str.replace(r"\s+", " ", regex=True)
              .str.strip()
              .replace({"nan": np.nan, "None": np.nan, "": np.nan}))

def to_dt(s: pd.Series):
    return pd.to_datetime(s, errors="coerce")

def to_num(s: pd.Series):
    return pd.to_numeric(s, errors="coerce")

def stable_hash_row(vals):
    text = "|".join(["" if pd.isna(v) else str(v) for v in vals])
    return hashlib.md5(text.encode("utf-8")).hexdigest()

def make_surrogate_key(series: pd.Series):
    codes, _ = pd.factorize(series.astype(str), sort=True)
    return codes + 1

def _col_or_blank(df, col):
    if col in df.columns:
        return df[col]
    return pd.Series([""] * len(df), index=df.index)

def build_customer_name_if_needed(df):
    if "Customer Name" not in df.columns:
        if ("Customer First Name" in df.columns) or ("Customer Last Name" in df.columns) or ("Customer Last name" in df.columns):
            title  = _col_or_blank(df, "Customer Title")
            first  = _col_or_blank(df, "Customer First Name")
            middle = _col_or_blank(df, "Customer Middle Name")
            last   = _col_or_blank(df, "Customer Last Name")
            if "Customer Last name" in df.columns and "Customer Last Name" not in df.columns:
                last = df["Customer Last name"]
            name = (
                title.astype(str).fillna("") + " " +
                first.astype(str).fillna("") + " " +
                middle.astype(str).fillna("") + " " +
                last.astype(str).fillna("")
            )
            df["Customer Name"] = name.str.replace(r"\s+", " ", regex=True).str.strip()
    return df

def standardize_one_df(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]
    df = build_customer_name_if_needed(df)

    for c in CANON_RAW_COLS_26:
        if c not in df.columns:
            df[c] = np.nan

    df = df[CANON_RAW_COLS_26].copy()

    # IMPORTANT: Policy_Id is string, so include in text cleaning (not numeric)
    text_cols = [
        "Customer Name","Customer_Segment","Maritial_Status","Gender",
        "Policy_Type","Policy_Type_Desc","Policy_Id","Policy_Name",
        "Policy_Term","Country","Region","State or Province","City","Postal Code"
    ]
    for c in text_cols:
        df[c] = norm_space(df[c])

    # Numeric (Policy_Id NOT here)
    df["Customer ID"] = to_num(df["Customer ID"])
    df["Policy_Type_Id"] = to_num(df["Policy_Type_Id"])
    df["Premium_Amt"] = to_num(df["Premium_Amt"])
    df["Total_Policy_Amt"] = to_num(df["Total_Policy_Amt"])
    df["Premium_Amt_Paid_TillDate"] = to_num(df["Premium_Amt_Paid_TillDate"])

    # Dates
    date_cols = ["DOB","Effective_Start_Dt","Effective_End_Dt",
                 "Policy_Start_Dt","Policy_End_Dt","Next_Premium_Dt","Actual_Premium_Paid_Dt"]
    for c in date_cols:
        df[c] = to_dt(df[c])

    # Keep rows that have keys
    df = df.dropna(subset=["Customer ID"]).copy()
    df = df[df["Policy_Id"].notna()].copy()

    df = df.drop_duplicates()
    return df

def load_all_sources(extract_base):
    all_rows = []
    csvs = glob.glob(os.path.join(extract_base, "**", "*.csv"), recursive=True)
    for f in sorted(csvs):
        df0 = read_csv_safe(f)
        df = standardize_one_df(df0)

        bn = os.path.basename(f)
        m = re.search(r"Insurance_details_US_(\w+)_day(\d+)\.csv", bn, flags=re.IGNORECASE)
        src_region = m.group(1) if m else "UNKNOWN"
        src_day = int(m.group(2)) if m else -1

        df["src_region_file"] = src_region
        df["snapshot_day"] = src_day
        df["src_file"] = bn

        all_rows.append(df)

    merged = pd.concat(all_rows, ignore_index=True) if all_rows else pd.DataFrame()
    merged = merged.drop_duplicates(subset=CANON_RAW_COLS_26 + ["src_region_file","snapshot_day"], ignore_index=True)
    return merged

def build_scd2(df, entity_col, tracked_cols, day_col="snapshot_day"):
    tracked_cols = list(tracked_cols)
    d = df[[entity_col, day_col] + tracked_cols].copy()
    d = d.loc[:, ~d.columns.duplicated()].copy()
    d = d.sort_values([entity_col, day_col]).reset_index(drop=True)

    if len(tracked_cols) == 0:
        d["_hash"] = 0
    else:
        tmp = d[tracked_cols].copy()
        for c in tracked_cols:
            if pd.api.types.is_datetime64_any_dtype(tmp[c]):
                tmp[c] = tmp[c].dt.strftime("%Y-%m-%d %H:%M:%S").fillna("")
            else:
                tmp[c] = tmp[c].astype("string").fillna("")
        d["_hash"] = pd.util.hash_pandas_object(tmp, index=False)

    d["_prev_hash"] = d.groupby(entity_col)["_hash"].shift(1)
    d["_is_change"] = (d["_hash"] != d["_prev_hash"]).fillna(True)

    ver = d[d["_is_change"]].copy()
    ver = ver.sort_values([entity_col, day_col]).reset_index(drop=True)
    ver["start_day"] = ver[day_col]
    ver["end_day"] = ver.groupby(entity_col)["start_day"].shift(-1)
    ver["is_current"] = ver["end_day"].isna()
    return ver.drop(columns=["_prev_hash","_is_change"])

def assign_scd_sk(trans_df, dim_scd_df, entity_col, day_col, sk_col):
    t = trans_df.copy()
    t[sk_col] = np.nan
    dim = dim_scd_df[[entity_col, "start_day", "end_day", sk_col]].copy()
    dim = dim.sort_values([entity_col, "start_day"])

    for key, grp in t.groupby(entity_col, sort=False):
        gidx = grp.index
        days = grp[day_col].values
        dv = dim[dim[entity_col] == key]
        if dv.empty:
            continue
        starts = dv["start_day"].values
        sks = dv[sk_col].values
        pos = np.searchsorted(starts, days, side="right") - 1
        pos = np.clip(pos, 0, len(starts)-1)
        t.loc[gidx, sk_col] = sks[pos]
    return t

def build_dwh(merged_raw: pd.DataFrame):
    df = merged_raw.rename(columns=RAW_TO_SNAKE).copy()
    df = df.loc[:, ~df.columns.duplicated()].copy()

    df["customer_id"] = df["customer_id"].astype("Int64")
    df["policy_type_id"] = df["policy_type_id"].astype("Int64")
    df["policy_id"] = df["policy_id"].astype("string")

    # Make policy natural key unique across regions
    df["policy_nk"] = df["src_region_file"].astype(str) + "|" + df["policy_id"].astype(str)

    df["address_nk"] = df.apply(lambda r: stable_hash_row([
        r["customer_id"], r["country"], r["region"], r["state_or_province"], r["city"], r["postal_code"]
    ]), axis=1)

    dim_policy_type = df[["policy_type_id","policy_type","policy_type_desc"]].drop_duplicates().copy()
    dim_policy_type["policy_type_sk"] = make_surrogate_key(dim_policy_type["policy_type_id"].astype(str))
    dim_policy_type = dim_policy_type[["policy_type_sk","policy_type_id","policy_type","policy_type_desc"]]

    cust_tracked = ["customer_name","customer_segment","marital_status","gender","dob"]
    cust_base = df[["customer_id","snapshot_day"] + cust_tracked].copy()
    cust_scd = build_scd2(cust_base, "customer_id", cust_tracked, "snapshot_day")
    cust_scd["customer_sk"] = make_surrogate_key(
        cust_scd["customer_id"].astype(str) + "|" + cust_scd["start_day"].astype(int).astype(str)
    )
    dim_customer = cust_scd[[
        "customer_sk","customer_id","customer_name","customer_segment","marital_status","gender","dob",
        "start_day","end_day","is_current"
    ]].copy()

    dim_address = df[["address_nk","customer_id","country","region","state_or_province","city","postal_code"]].drop_duplicates().copy()
    dim_address["address_sk"] = make_surrogate_key(dim_address["address_nk"])
    dim_address = dim_address[[
        "address_sk","address_nk","customer_id","country","region","state_or_province","city","postal_code"
    ]]

    pol_tracked = ["policy_id","customer_id","policy_name","policy_term","policy_start_dt","policy_end_dt","policy_type_id","premium_amt"]
    pol_base = df[["policy_nk","snapshot_day"] + pol_tracked].copy()
    pol_scd = build_scd2(pol_base, "policy_nk", pol_tracked, "snapshot_day")
    pol_scd["policy_sk"] = make_surrogate_key(
        pol_scd["policy_nk"].astype(str) + "|" + pol_scd["start_day"].astype(int).astype(str)
    )
    pol_scd = pol_scd.merge(dim_policy_type, on="policy_type_id", how="left")
    dim_policy = pol_scd[[
        "policy_sk","policy_nk","policy_id","customer_id",
        "policy_name","policy_term","policy_start_dt","policy_end_dt",
        "policy_type_sk","policy_type_id","premium_amt",
        "start_day","end_day","is_current"
    ]].copy()

    fact = df.copy()
    fact = fact.merge(dim_policy_type, on="policy_type_id", how="left")
    fact = fact.merge(dim_address[["address_nk","address_sk"]], on="address_nk", how="left")
    fact = assign_scd_sk(fact, dim_customer[["customer_id","start_day","end_day","customer_sk"]], "customer_id", "snapshot_day", "customer_sk")
    fact = assign_scd_sk(fact, dim_policy[["policy_nk","start_day","end_day","policy_sk"]], "policy_nk", "snapshot_day", "policy_sk")

    fact["fact_sk"] = np.arange(1, len(fact) + 1)

    fact_transactions = fact[[
        "fact_sk",
        "snapshot_day","src_region_file","src_file",
        "customer_sk","policy_sk","policy_type_sk","address_sk",
        "total_policy_amt","premium_amt","premium_amt_paid_tilldate",
        "next_premium_dt","actual_premium_paid_dt"
    ]].copy()

    return dim_customer, dim_address, dim_policy, dim_policy_type, fact_transactions

# ---------------------------
# RUN
# ---------------------------
zip_paths = find_zip_files()
print("Found zips:", zip_paths)

extract_all_zips(zip_paths, EXTRACT_DIR)
merged_raw = load_all_sources(EXTRACT_DIR)

print("Merged rows:", len(merged_raw))

merged_26 = merged_raw[CANON_RAW_COLS_26].copy()
merged_26.to_csv(os.path.join(OUT_DIR, "merged_clean_26cols.csv"), index=False)

dim_customer, dim_address, dim_policy, dim_policy_type, fact_transactions = build_dwh(merged_raw)

dim_customer.to_csv(os.path.join(OUT_DIR, "dim_customer.csv"), index=False)
dim_address.to_csv(os.path.join(OUT_DIR, "dim_address.csv"), index=False)
dim_policy_type.to_csv(os.path.join(OUT_DIR, "dim_policy_type.csv"), index=False)
dim_policy.to_csv(os.path.join(OUT_DIR, "dim_policy.csv"), index=False)
fact_transactions.to_csv(os.path.join(OUT_DIR, "fact_transactions.csv"), index=False)

print("\nâœ… DONE. Files in:", OUT_DIR)
print("Row counts:")
print("merged_26:", len(merged_26))
print("dim_customer:", len(dim_customer))
print("dim_address:", len(dim_address))
print("dim_policy_type:", len(dim_policy_type))
print("dim_policy:", len(dim_policy))
print("fact_transactions:", len(fact_transactions))