# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fG6D6pz7PjvPwwH1v62c8wufGd76qGXa
"""

# =========================
# Q2 (Insurance Policy) - Colab Ready (b, c, d, e, f)
# Upload the 4 region ZIPs into Colab (/content) and run.
# =========================

import os, re, glob, zipfile
import pandas as pd
import numpy as np

WORKDIR = "/content"
EXTRACT_DIR = os.path.join(WORKDIR, "insurance_extracted")
OUT_DIR = os.path.join(WORKDIR, "q2_outputs")
os.makedirs(EXTRACT_DIR, exist_ok=True)
os.makedirs(OUT_DIR, exist_ok=True)

# ---------- 1) Unzip ----------
zip_paths = sorted(glob.glob(os.path.join(WORKDIR, "*.zip")))
if not zip_paths:
    raise FileNotFoundError("No .zip files found in /content. Upload the 4 region ZIPs and re-run.")

for zp in zip_paths:
    with zipfile.ZipFile(zp, "r") as z:
        z.extractall(EXTRACT_DIR)

csv_paths = sorted(glob.glob(os.path.join(EXTRACT_DIR, "*.csv")))
if not csv_paths:
    raise FileNotFoundError("No .csv extracted. Verify ZIP contents.")

print("ZIPs:", [os.path.basename(z) for z in zip_paths])
print("CSVs:", len(csv_paths))

# ---------- 2) Helpers ----------
def normalize_col(c: str) -> str:
    c = str(c).strip().replace("/", "_")
    c = re.sub(r"\s+", "_", c)
    c = re.sub(r"[^0-9A-Za-z_]+", "", c)
    return c.lower()

def extract_day(fname: str) -> int:
    m = re.search(r"_day(\d+)\.csv$", fname, flags=re.IGNORECASE)
    return int(m.group(1)) if m else -1

def extract_region(fname: str) -> str:
    m = re.search(r"_us_([a-z]+)_day\d+\.csv$", fname, flags=re.IGNORECASE)
    return m.group(1).capitalize() if m else ""

def read_one_csv(path: str) -> pd.DataFrame:
    d = pd.read_csv(path)
    base = os.path.basename(path)
    d["__file"] = base
    d["__day"] = extract_day(base)
    d["__region_from_file"] = extract_region(base)
    return d

def coalesce_duplicate_columns(raw: pd.DataFrame) -> pd.DataFrame:
    cols = list(raw.columns)
    groups = {}
    for i, c in enumerate(cols):
        groups.setdefault(c, []).append(i)
    out = pd.DataFrame(index=raw.index)
    for c, idxs in groups.items():
        if len(idxs) == 1:
            out[c] = raw.iloc[:, idxs[0]]
        else:
            sub = raw.iloc[:, idxs]
            out[c] = sub.bfill(axis=1).iloc[:, 0]
    return out

def pick_first_existing(df: pd.DataFrame, candidates: list[str]):
    for c in candidates:
        if c in df.columns:
            return c
    return None

# ---------- 3) Load + normalize ----------
raw = pd.concat([read_one_csv(p) for p in csv_paths], ignore_index=True, sort=False)
raw = raw.rename(columns={c: normalize_col(c) for c in raw.columns})
df = coalesce_duplicate_columns(raw)

# Fix typo
if "maritial_status" in df.columns and "marital_status" not in df.columns:
    df["marital_status"] = df["maritial_status"]

# Canonical mapping
colmap = {
    "customer_id": ["customer_id", "customerid", "cust_id", "custid"],
    "policy_id": ["policy_id", "policyid"],
    "policy_type": ["policy_type", "policytype"],
    "policy_type_id": ["policy_type_id", "policytypeid", "policy_typeid"],
    "policy_term": ["policy_term", "policyterm"],
    "total_policy_amt": ["total_policy_amt", "totalpolicyamt", "total_policy_amount"],
    "policy_start_dt": ["policy_start_dt", "policystartdt", "policy_start_date"],
    "dob": ["dob", "date_of_birth"],
    "region": ["region"],
    "customer_title": ["customer_title", "customertitle"],
    "customer_first_name": ["customer_first_name", "customerfirstname", "first_name", "firstname"],
    "customer_middle_name": ["customer_middle_name", "customermiddlename", "middle_name", "middlename"],
    "customer_last_name": ["customer_last_name", "customerlastname", "last_name", "lastname"],
    "customer_segment": ["customer_segment", "customersegment"],
    "marital_status": ["marital_status", "maritial_status"],
    "policy_type_desc": ["policy_type_desc", "policytypedesc", "policy_type_description"],
}

for canon, candidates in colmap.items():
    src = pick_first_existing(df, candidates)
    if src is None:
        df[canon] = np.nan
    elif src != canon:
        df[canon] = df[src]

# Region fallback from filename
df["region"] = df["region"].astype(str).str.strip()
mask = df["region"].isna() | df["region"].str.lower().isin(["nan", "none", ""])
df.loc[mask, "region"] = df.loc[mask, "__region_from_file"]

# Numeric parsing
for c in ["customer_id", "policy_type_id", "total_policy_amt"]:
    df[c] = pd.to_numeric(df[c], errors="coerce")

# Policy id as string
df["policy_id"] = df["policy_id"].astype(str).str.strip()

# Parse dates
for c in ["dob", "policy_start_dt"]:
    df[c] = pd.to_datetime(df[c], errors="coerce")

# Build customer_name
def build_name(r):
    parts = []
    for k in ["customer_title", "customer_first_name", "customer_middle_name", "customer_last_name"]:
        v = r.get(k)
        if pd.notna(v) and str(v).strip() and str(v).strip().lower() not in ["nan", "none"]:
            parts.append(str(v).strip())
    s = " ".join(parts).strip()
    return s if s else np.nan

df["customer_name"] = df.apply(build_name, axis=1)

# ---------- 4) Strong validity filter ----------
valid = df[
    df["customer_id"].notna() &
    df["policy_type_id"].notna() &
    df["policy_id"].notna() &
    (df["policy_id"].astype(str).str.lower() != "nan")
].copy()

# Hard stop for NaNs
if valid["customer_id"].isna().any():
    bad = valid[valid["customer_id"].isna()]
    display(bad.head(20))
    raise ValueError("customer_id has NaN after validity filter.")

valid["customer_id"] = valid["customer_id"].astype(int)

# ---------- 5) Current snapshot per policy_id ----------
cur = valid.sort_values(["policy_id", "__day"]).groupby("policy_id", as_index=False).tail(1).copy()

# ---------- 6) Per-customer per-day snapshot ----------
snap = (
    valid.sort_values(["customer_id", "__day", "policy_start_dt", "policy_id"])
         .groupby(["customer_id", "__day"], as_index=False).tail(1)
         .sort_values(["customer_id", "__day"])
         .copy()
)

# Fill identity fields
id_cols = ["customer_title","customer_first_name","customer_last_name","customer_segment","customer_name","policy_type_desc"]
for c in id_cols:
    if c not in snap.columns:
        snap[c] = np.nan
snap[id_cols] = snap.groupby("customer_id")[id_cols].ffill().bfill()

# ---- (b) Customers who changed policy type ----
b_rows = []
for cid, g in snap.groupby("customer_id"):
    if g["policy_type"].nunique(dropna=True) <= 1:
        continue
    g = g.reset_index(drop=True)
    cur_row = g.iloc[-1]
    prev_row = None
    for i in range(len(g)-2, -1, -1):
        if g.iloc[i]["policy_type"] != cur_row["policy_type"]:
            prev_row = g.iloc[i]
            break
    if prev_row is None:
        continue
    b_rows.append({
        "Customer Id": int(cur_row["customer_id"]),
        "Customer Name": cur_row.get("customer_name", np.nan),
        "Previous Policy Type": prev_row["policy_type"],
        "Previous Policy Type Id": prev_row["policy_type_id"],
        "Previous Policy Type Desc": prev_row.get("policy_type_desc", np.nan),
        "Current Policy Type": cur_row["policy_type"],
        "Current Policy Type Id": cur_row["policy_type_id"],
        "Current Policy Type Desc": cur_row.get("policy_type_desc", np.nan),
    })

q_b = pd.DataFrame(b_rows).sort_values(["Customer Id"]).reset_index(drop=True)

# ---- (c) Total policy amount by all customers ----
q_c = cur.groupby(["customer_id","customer_name"], as_index=False)["total_policy_amt"].sum()
q_c = q_c.rename(columns={
    "customer_id":"Customer Id",
    "customer_name":"Customer Name",
    "total_policy_amt":"Total_Policy_Amt"
})
q_c["Region"] = "All"
q_c = q_c.sort_values("Total_Policy_Amt", ascending=False).reset_index(drop=True)

# ---- (d) Total policy amt for Auto ----
auto = cur[cur["policy_type"].astype(str).str.lower() == "auto"]
q_d = auto.groupby(["customer_id","customer_name"], as_index=False)["total_policy_amt"].sum()
q_d = q_d.rename(columns={
    "customer_id":"Customer Id",
    "customer_name":"Customer Name",
    "total_policy_amt":"Total_Policy_Amt"
})
q_d["Region"] = "All"
q_d["Policy Type"] = "Auto"
q_d = q_d.sort_values("Total_Policy_Amt", ascending=False).reset_index(drop=True)

# ---- (e) East & West, Quarterly, 2012 ----
ew = cur[
    cur["region"].isin(["East","West"]) &
    (cur["policy_term"].astype(str).str.lower() == "quarterly") &
    (cur["policy_start_dt"].dt.year == 2012)
].copy()

q_e = ew.groupby(["customer_id","customer_name"], as_index=False).agg(
    Policy_Start_Dt=("policy_start_dt","min"),
    Total_Policy_Amt=("total_policy_amt","sum")
)

q_e = q_e.rename(columns={
    "customer_id":"Customer Id",
    "customer_name":"Customer Name"
})

q_e["Region"] = "East and West"
q_e["Policy_Term"] = "Quarterly"
q_e = q_e.sort_values("Total_Policy_Amt", ascending=False).reset_index(drop=True)

# ---- (f) Marital status change ----
BASE_INGEST_DATE = pd.Timestamp("2024-04-11")
FAR_FUTURE = pd.Timestamp("2099-12-31")

snap_m = snap.copy()
snap_m["ingest_dt"] = BASE_INGEST_DATE + pd.to_timedelta(snap_m["__day"], unit="D")

if snap_m["customer_id"].isna().any():
    bad = snap_m[snap_m["customer_id"].isna()]
    display(bad.head(20))
    raise ValueError("NaN customer_id in snap_m")

f_rows = []
for cid, g in snap_m.sort_values(["customer_id", "__day"]).groupby("customer_id"):
    ms = g["marital_status"].astype(str).str.strip()
    ms = ms.replace({"nan": np.nan, "None": np.nan, "": np.nan})
    uniq = pd.Series(ms.dropna().unique())
    if len(uniq) <= 1:
        continue

    g = g.reset_index(drop=True)
    start_idx = 0
    for i in range(1, len(g)):
        if str(g.loc[i-1, "marital_status"]).strip() != str(g.loc[i, "marital_status"]).strip():
            seg = g.iloc[start_idx:i]
            r0 = seg.iloc[0]
            f_rows.append({
                "Customer ID": int(r0["customer_id"]),
                "Customer Name": r0.get("customer_name", np.nan),
                "Customer Title": r0.get("customer_title", np.nan),
                "Customer First Name": r0.get("customer_first_name", np.nan),
                "Customer Last Name": r0.get("customer_last_name", np.nan),
                "Customer Segment": r0.get("customer_segment", np.nan),
                "Marital Status": str(seg.iloc[0]["marital_status"]).strip(),
                "Start_Dt_Marital_Status": pd.to_datetime(seg.iloc[0]["ingest_dt"]).date(),
                "End_Dt_Marital_Status": pd.to_datetime(g.loc[i, "ingest_dt"]).date(),
            })
            start_idx = i

    seg = g.iloc[start_idx:]
    r0 = seg.iloc[0]
    f_rows.append({
        "Customer ID": int(r0["customer_id"]),
        "Customer Name": r0.get("customer_name", np.nan),
        "Customer Title": r0.get("customer_title", np.nan),
        "Customer First Name": r0.get("customer_first_name", np.nan),
        "Customer Last Name": r0.get("customer_last_name", np.nan),
        "Customer Segment": r0.get("customer_segment", np.nan),
        "Marital Status": str(seg.iloc[0]["marital_status"]).strip(),
        "Start_Dt_Marital_Status": pd.to_datetime(seg.iloc[0]["ingest_dt"]).date(),
        "End_Dt_Marital_Status": FAR_FUTURE.date(),
    })

q_f = pd.DataFrame(f_rows).sort_values(["Customer ID","Start_Dt_Marital_Status"]).reset_index(drop=True)

# ---------- 7) Display ----------
pd.set_option("display.max_columns", 200)

print("\n===== (b) Customers who changed Policy Type =====")
display(q_b.head(50))

print("\n===== (c) Total policy amount by all customers and all regions =====")
display(q_c.head(50))

print("\n===== (d) Total policy amount by customers with Policy Type = Auto =====")
display(q_d.head(50))

print("\n===== (e) East+West, Quarterly term, year 2012 =====")
display(q_e.head(50))

print("\n===== (f) Customers whose marital status has changed =====")
display(q_f.head(50))

# ---------- 8) Save outputs ----------
q_b.to_csv(os.path.join(OUT_DIR, "q2_b_policy_type_changed.csv"), index=False)
q_c.to_csv(os.path.join(OUT_DIR, "q2_c_total_policy_amt_all.csv"), index=False)
q_d.to_csv(os.path.join(OUT_DIR, "q2_d_total_policy_amt_auto.csv"), index=False)
q_e.to_csv(os.path.join(OUT_DIR, "q2_e_east_west_quarterly_2012.csv"), index=False)
q_f.to_csv(os.path.join(OUT_DIR, "q2_f_marital_status_changed.csv"), index=False)

print("\nSaved CSV outputs to:", OUT_DIR)